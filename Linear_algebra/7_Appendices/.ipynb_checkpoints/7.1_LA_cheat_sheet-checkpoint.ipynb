{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d617ad73",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "source": [
    "# Linear Algebra Cheat Sheet\n",
    "\n",
    "\n",
    "## Matrices\n",
    "\n",
    "A matrix is a rectangular array of elements. The if a matrix $A$ has $m$ rows and $n$ columns then the elements of $A$ are [**indexed**](indexing-a-matrix-section) by $a_{ij}$ where $i$ denotes the row and $j$ denotes the column\n",
    "\n",
    "\\begin{align*} \n",
    "    \\begin{pmatrix} \n",
    "        a_{11} & a_{12} & \\cdots & a_{1m} \\\\  \n",
    "        a_{21} & a_{22} & \\cdots & a_{2m} \\\\ \n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "        a_{m1} & a_{m2} & \\cdots & a_{mn} \n",
    "    \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "### Matrix arithmetic \n",
    "\n",
    "Let $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$, $B = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}$ and $C = \\begin{pmatrix} 1 & 0 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{pmatrix}$.\n",
    "\n",
    "- [**Matrix equality**](matrix-equality-definition): Two matrices $A$ and $B$ are equal if $[A]_{ij} = [B]_{ij}$ for all $i = 1, \\ldots, m$ and $j = 1, \\ldots n$.\n",
    " \n",
    "- [**Addition/subtraction**](matrix-addition-definition): $[A \\pm B]_{ij} = [A]_{ij} \\pm [B]_{ij}$\n",
    "\\begin{align*} \n",
    "    A + B = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} + \n",
    "    \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}\n",
    "    = \\begin{pmatrix} 6 & 8 \\\\ 10 & 12 \\end{pmatrix} \n",
    "\\end{align*}\n",
    "\n",
    "- [**Multiplication by a scalar**](scalar-multipication-of-a-matrix-definition): $k[A]_{ij} = [kA]_{ij}$\n",
    "\\begin{align*} \n",
    "    3A = 3 \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} = \\begin{pmatrix} 3 & 6 \\\\ 9 & 12 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Multiplication of two matrices**](matrix-multiplication-definition): $[AB]_{ij} = \\displaystyle\\sum_{k=1}^n [A]_{ik}[B]_{jk}$\n",
    "\\begin{align*}\n",
    "    AB &= \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \n",
    "    \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix} \\\\\n",
    "    &= \\begin{pmatrix} 5 + 14 & 6 + 16 \\\\ 15 + 28 & 18 + 32 \\end{pmatrix} \\\\\n",
    "    &= \\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Matrix exponent**](matrix-exponents-definition): $A^k = \\underbrace{AA \\cdots A}_{k}$\n",
    "\\begin{align*}\n",
    "    A^2 = AA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\n",
    "    = \\begin{pmatrix} 7 & 10 \\\\ 15 & 22 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Matrix transpose**](matrix-transpose-definition): $[A^\\mathrm{T}]_{ij} = [A]_{ji}$\n",
    "\\begin{align*}\n",
    "    A^\\mathrm{T} = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}^\\mathrm{T} = \n",
    "    \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "### Special matrices\n",
    "\n",
    "- [**Diagonal matrix**](diagonal-matrix-definition): $[D]_{ij} = 0$ for $i, j = 1, \\ldots, n$ where $i \\neq j $\n",
    "\n",
    "\\begin{align*} \n",
    "    \\begin{pmatrix} \n",
    "      a & 0 & 0 \\\\ \n",
    "      0 & b & 0 \\\\ \n",
    "      0 & 0 & c\n",
    "    \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Zero matrix**](zero-matrix-definition): $[\\mathbf{0}]_{ij} = 0$ for all $i = 1, \\ldots, m$ and $j = 1, \\ldots, n$\n",
    "\n",
    "\\begin{align*} \n",
    "    \\mathbf{0}_{2\\times 3} = \\begin{pmatrix} \n",
    "      0 & 0 & 0 \\\\ \n",
    "      0 & 0  & 0 \n",
    "    \\end{pmatrix}\n",
    "\\end{align*}\n",
    "    \n",
    "- [**Identity matrix**](identity-matrix-definition): For an $n \\times n$ matrix $A$, is identity matrix is an $n\\times n$ matrix $I$ such that $AI = IA = A$ where $[I]_{ij} = \\begin{cases} 1 & i = j, \\\\ 0, & i \\neq j \\end{cases}$ \n",
    "\n",
    "\\begin{align*}\n",
    "    I_3 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Symmetric matrix**](symmetric-matrix-definition): An $n \\times n$ matrix is symmetric if $[A]_{ij} = [A]_{ji}$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{pmatrix} a & b & c \\\\ b & d & e \\\\ c & e & f \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "### Determinants\n",
    "\n",
    "- [**Determinant of a $2\\times 2$ matrix**](2x2-determinant-definition): $\\det \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} = ad - bc$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det(A) = \\begin{vmatrix} 1 & 2 \\\\ 3 & 4 \\end{vmatrix} = 1(4) - 2(3) = -2\n",
    "\\end{align*}\n",
    "\n",
    "- [**Minor**](minor-definition): For an $n\\times n$ matrix $A$, the minor $M_{ij}$ is the determinant of the matrix formed by omitting row $i$ and column $j$ from $A$\n",
    "\n",
    "- [**Cofactor**](cofactor-definition): For an $n\\times n$ matrix $A$ the cofactor is $C_{ij} = (-1)^{i+j} M_{ij}$\n",
    "\n",
    "- [**Determinant of an $n \\times n$ matrix**](nxn-determinant-example): $\\det(A) = \\displaystyle\\sum_{i = 1}^n a_{ij} C_{ij}$ for $j \\in \\{1, \\ldots, n\\}$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det(C) &= 1 \\begin{vmatrix} 5 & 6 \\\\ 8 & 9 \\end{vmatrix} - 0 \n",
    "    \\begin{vmatrix} 4 & 6 \\\\ 7 & 9 \\end{vmatrix} + 3 \n",
    "    \\begin{vmatrix} 4 & 5 \\\\ 7 & 8 \\end{vmatrix} \\\\\n",
    "    &= 1(45 - 48) + 0(36 - 42) + 3(32 - 35) \\\\\n",
    "    &= -12\n",
    "\\end{align*}\n",
    "\n",
    "### Matrix inverse\n",
    "\n",
    "- [**Inverse matrix**](inverse-matrix-definition): For an $n \\times n$ matrix $A$ such that $\\det(A) \\neq 0$ then the inverse matrix is and $n \\times n$ matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.\n",
    "\n",
    "- [**Adjoint matrix**](adjoint-definition): For an $n \\times n$ matrix $A$ the adjoint matrix is $\\operatorname{adj}(A) = C^T$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\operatorname{adj}(A) = \\begin{pmatrix} 4 & -3 \\\\ -2 & 1 \\end{pmatrix}^\\mathrm{T} =\n",
    "    \\begin{pmatrix} 4 & -3 \\\\ -2 & 1 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**The adjoint-determinant formula**](adjoint-determinant-formula-definition): $A^{-1} = \\dfrac{\\operatorname{adj}(A)}{\\det(A)}$\n",
    "\n",
    "\\begin{align*}\n",
    "    A^{-1} = \\dfrac{1}{-2} \\begin{pmatrix} 4 & -3 \\\\ -2 & 1 \\end{pmatrix}\n",
    "    = \\begin{pmatrix} -2 & \\frac{3}{2} \\\\ 1 & -\\frac{1}{2} \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7163b",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "source": [
    "## Systems of linear equations\n",
    "\n",
    "A [system of linear equations](system-of-linear-equation-definition) can be expressed as the matrix equation $A \\mathbf{x} = \\mathbf{b}$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{pmatrix}\n",
    "        a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    "        a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} =\n",
    "    \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{pmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "where $A$ is the coefficient matrix, $\\mathbf{x}$ is the variable vector and $\\mathbf{b}$ is the constant vector. \n",
    "\n",
    "### Row reduction\n",
    "\n",
    "The following [**elementary row operations**](ero-definition) can be applied to a system without changing the solution\n",
    "\n",
    "- **Swap rows $i$ and $j$**: &emsp;  $R_i \\leftrightarrow R_j$\n",
    "- **Multiply row $i$ by $k$**:  &emsp; $kR_i$\n",
    "- **Add row $j$ multiplied by $k$ to row $i$**:  &emsp; $R_i + kR_j$\n",
    "\n",
    "Elementary row operations can be used to reduce a matrix to row echelon form.\n",
    "\n",
    "- **Pivot element**: The first non-zero element in the row.\n",
    "\n",
    "- [**Row Echelon Form (REF)**](ref-definition): In each non-zero row the pivot element is to the right of the pivot element of the row above.\n",
    "\n",
    "- [**Row reduction to REF**](ge-definition): To row reduce an $m \\times n$ matrix $A$ to reduced row echelon form we do the following:\n",
    "\n",
    "    1. Initalise the pivot row to $i=1$ and pivot column to $k=1$ \n",
    "    2. If $a_{ik} = 0$ perform a row swap with a row beneath the pivot row $i$ with a non-zero element in the pivot column $k$. If no such rows exist set $k = k + 1$ and repeat step 2.\n",
    "    3. For each row $j = i+1 \\ldots m$ beneath the pivot row subtract the pivot row $i$ multiplied by $\\dfrac{a_{jk}}{a_{ik}}$ from row $j$. \n",
    "    4. Set $i = 1 + 1$ and $k = k + 1$ and repeat steps 2 and 3 until $i > m$ or $k > n$.\n",
    "\n",
    "- [**Partial pivoting**](ge-pp-definition): A row swap is performed so that the pivot element has a larger absolute value than the elements in the column below it.  \n",
    "\n",
    "- [**Row reduction using partial pivoting**](ge-pp-definition): To row reduce an $m \\times n$ matrix $A$ to reduced row echelon form using partial pivoting we do the following:\n",
    "\n",
    "    1. Initalise the pivot row to $i=1$ and pivot column to $k=1$\n",
    "    2. Swap the pivot row $i$ with the row below which has the largest absolute value element in the pivot column $k$ which is greater than the absolute value of the pivot element $|a_{ik}|$. \n",
    "    3. If $a_{ik} = 0$ then set $k = k + 1$ and repeat step 2.\n",
    "    4. For each row $j = i+1 \\ldots m$ beneath the pivot row subtract the pivot row $i$ multiplied by $\\dfrac{a_{jk}}{a_{ik}}$ from row $j$. \n",
    "    5. Set $i = 1 + 1$ and $k = k + 1$ and repeat steps 2 to 4 until $i > m$ or $k > n$.\n",
    "\n",
    "- [**Reduced Row Echelon Form (RREF)**](rref-definition): The matrix is in row echelon form and the pivot element in each row has a value of 1 and it is the only non-zero element in its column.\n",
    "\n",
    "- [**Row reduction to RREF**](rref-definition):\n",
    "\n",
    "    1. Initalise the pivot row to $i=1$ and pivot column to $k=1$ \n",
    "    2. If $a_{ik} = 0$ perform a row swap with a row beneath the pivot row $i$ with a non-zero element in the pivot column $k$. If no such rows exist set $k = k + 1$ and repeat step 2.\n",
    "    3. Divide the pivot row $i$ by the value of the pivot element $a_{ik}$.\n",
    "    4. For each row $j = i \\ldots m$ where $i \\neq j$ subtract the pivot row $i$ multiplied by $a_{ik}$ from row $j$. \n",
    "    5. Set $i = 1 + 1$ and $k = k + 1$ and repeat steps 2 to 4 until $i > m$ or $k > n$.\n",
    "\n",
    "\n",
    "### Solutions methods for systems of linear equations\n",
    "\n",
    "The solution to a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ is the vector $\\mathbf{x}$ that satisfies this matrix equation. The following methods can be used to calculate the solution if it exsists.\n",
    "\n",
    "- [**Matrix inverse**](solution-using-inverse-matrix-theorem): $\\mathbf{x} = A^{-1} \\mathbf{b}$ \n",
    "\n",
    "- [**Cramer's rule**](cramers-rule-theorem): $x_i = \\dfrac{\\det(A_i)}{\\det(A)}$ where $A_i$ is a matrix formed by replacing column $i$ of $A$ with $\\mathbf{b}$. Note that if $\\det(A)=0$ then the system does not have a solution.\n",
    "\n",
    "- [**Gaussian elimination (GE)**](ge-definition): Form an augmented matrix $(A \\mid \\mathbf{b})$ and reduce to REF. The solution is then calculated using back substitution.\n",
    "\n",
    "- [**Gauss-Jordan Elimination (GJE)**](gje-definition):  Form an augmented matrix $(A \\mid \\mathbf{b})$ and reduce to RREF. The solution vector $\\mathbf{x}$ is contained to the right of the partition.\n",
    "\n",
    "- [**Calculating an inverse matrix using GJE**](gj-matrix-inverse-section): Form an augmented matrix $(A \\mid I)$ and row reduce to RREF. The inverse of $A$ is the matrix to the right-hand side of the partition. \n",
    "\n",
    "### Consistent, inconsistent and indeterminate systems\n",
    "\n",
    "- [**Rank of a matrix**](rank-definition): The rank of a matrix $\\rho(A)$ is the number of non-zero rows of the row echelon form of $A$.\n",
    "- [**Consistent system**](consistent-system-theorem): A system of linear equations is consistent and has a unique solution if $\\rho(A \\mid \\mathbf{b}) = \\rho(A)$.\n",
    "- [**Inconsistent system**](inconsistent-system-theorem): A system of linaer equations is inconsistent has does not have a solution if $\\rho(A \\mid \\mathbf{b}) > \\rho(A)$.\n",
    "- [**Indeterminate system**](indeterminate-system-theorem): A system of linear equation is indeterminate and has an infinite number of solutions if  $\\rho(A \\mid \\mathbf{b}) < \\rho(A)$.\n",
    "\n",
    "In the case of an intdeterminate system, a general solution can be found by assigning parameters to the free variables.\n",
    "\n",
    "---\n",
    "\n",
    "## Vectors\n",
    "\n",
    "For the following let $\\mathbf{a} = (1, 2, 3)$ and $\\mathbf{b} = (4, 5, 6)$\n",
    "\n",
    "- [**Vector**](vectors-section): A vector is an object which has direction and magnitude (length).\n",
    "- [**Position vector**](position-vector-section): A vector which has a tail at the origin.\n",
    "- [**Magnitude**](magnitude-definition): The magnitude of a vector $|\\mathbf{a}| = \\sqrt{\\displaystyle\\sum_{i=1}^n a_i^2}$.\n",
    "\n",
    "\\begin{align*}\n",
    "    |\\mathbf{a}| &= \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Unit vector**](unit-vector-definition): A vector with a magnitude of 1. A unit vector pointing in the same direction as $\\mathbf{a}$ is $\\hat{\\mathbf{a}} = \\dfrac{\\mathbf{a}}{|\\mathbf{a}|}$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{\\mathbf{a}} = \\frac{1}{\\sqrt{14}} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \n",
    "    \\begin{pmatrix} \\frac{1}{\\sqrt{14}} \\\\ \\frac{2}{\\sqrt{14}} \\\\ \\frac{3}{\\sqrt{14}} \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- [**Dot product**](dot-product-definition): The dot product between two vectors is a scalar value calculated using $\\mathbf{a} \\cdot \\mathbf{b} = \\displaystyle\\sum_{i=1}^n a_ib_i$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{a} \\cdot \\mathbf{b} = 1(4) + 2(5) + 3(6) = 32\n",
    "\\end{align*}\n",
    "\n",
    "- [**Geometric definition of the dot product**](dot-product-definition): The dot product $\\mathbf{a} \\cdot \\mathbf{b}$ is related to $\\theta$ the angle between $\\mathbf{a}$ and $\\mathbf{b}$ by $\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$.\n",
    "\n",
    "- [**Cross product**](cross-product-definition): The cross product between two vectors in $\\mathbb{R}^3$ is a vector calculated using $\\mathbf{a} \\times \\mathbf{b} = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{vmatrix}$ where $\\mathbf{i} = (1, 0, 0)$, $\\mathbf{j} = (0, 1, 0)$ and $\\mathbf{k} = (0, 0, 1)$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{a} \\times \\mathbf{b} = \n",
    "    \\begin{vmatrix} \n",
    "        \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n",
    "        1 & 2 & 3 \\\\\n",
    "        4 & 5 & 6\n",
    "    \\end{vmatrix} =\n",
    "    -3\\mathbf{i} + 6\\mathbf{j} - 3\\mathbf{k} = \\begin{pmatrix} -3 \\\\ 6 \\\\ -3 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "## Co-ordinate geometry\n",
    "\n",
    "- [**Euclidean space**](euclidean-space-section): The position of an object in $n$-dimensional space, $\\mathbb{R}^n$, is defined as the signed distance along $n$ orthogonal (perpendicular) axis from the origin. These distances are expressed in an $n$-tuple, e.g., $(x, y, z)$ for $\\mathbb{R}^3$.\n",
    "\n",
    "- [**Point**](points-section): A point is an object that has no dimension.\n",
    "\n",
    "- [**Line**](lines-section): A line is a one dimensional object which has length but no breath.\n",
    "\n",
    "- [**Plane**](planes-section): A plane is a two dimensional object with length and breadth.\n",
    "\n",
    "### Lines\n",
    "\n",
    "For the following let $\\ell_1: P_1 + t\\mathbf{d}_1$ and $\\ell_2: P_2 + t\\mathbf{d}_2$.\n",
    "\n",
    "- [**Vector equation of a line**](vector-equation-of-a-line-definition): A point on the line $\\ell$ can be calculated using $Q = P + t\\mathbf{d}$ where $P$ is a point on the line, $t\\in \\mathbb{R}$ and $\\mathbf{d}$ is a vector indicating the direction of the line.\n",
    "\n",
    "- [**Intersecting lines**](line-line-intersection-definition): The two lines $\\ell_1$ and $\\ell_2$ intersect if there exists a value of $t$ such that $P_1 + t\\mathbf{d}_1 = P_2 + t\\mathbf{d}_2$.\n",
    "\n",
    "- [**Parallel lines**](parallel-lines-definition): The two lines $\\ell_1$ and $\\ell_2$  are parallel if there exists a value $k$ such that $\\mathbf{d}_1 = k\\mathbf{d}_2$.\n",
    "\n",
    "- [**Skew lines**](skew-lines-definition): The two lines $\\ell_1$ and $\\ell_2$ are skew if they do not intersect and are not parallel.\n",
    "\n",
    "- [**Perpendicular lines**](perpendicular-lines-definition): The two lines $\\ell_1$ and $\\ell_2$ are perpendicular if $\\mathbf{d}_1$ and $\\mathbf{d}_2$ are at right angles, i.e., $\\mathbf{d}_1 \\cdot \\mathbf{d}_2 = 0$.\n",
    "\n",
    "### Planes\n",
    "\n",
    "- **Normal vector**: The normal vector to a plane is a vector which is perpendicular to every vector that lies on the plane, and is calculated using $\\mathbf{n} = \\mathbf{d}_1 \\times \\mathbf{d}_2$.\n",
    "\n",
    "- [**Point normal equation**](point-normal-definition): A plane which passes through the point with co-ordinates $(x_0, y_0, z_0)$ and has a normal vector $\\mathbf{n}$ can be described using $\\mathbf{n} \\cdot (x - x_0, y - y_0, z - z_0) = 0$.\n",
    "\n",
    "- [**Intersection between three planes**](intersection-of-planes-section): Three planes in $\\mathbb{R}^n$ can instersect at a single point, on a single line or not intersect at all.\n",
    "\n",
    "### Shortest distance problems\n",
    "\n",
    "- [**Between two points**](shortest-distance-between-two-points): The shortest distance between the two points $P = (p_1, p_2, \\ldots, p_n)$ and $Q = (q_1, q_2, \\ldots, q_n)$ is $d = |Q - P|$.\n",
    "\n",
    "- [**Between a point and a line**](point-line-distance-theorem): The shortest distance between the point $Q$ and the line $\\ell:P + t\\mathbf{d}$ is $d = |Q - R|$ where $R = P + t\\mathbf{d}$ and $t = \\dfrac{(Q - P)\\cdot \\mathbf{d}}{\\mathbf{d} \\cdot \\mathbf{d}}$.\n",
    "\n",
    "- [**Between two lines**](line-line-distance-theorem): The shortest distances between the two skew lines $\\ell_1:P_1 + t\\mathbf{d}_1$ and $\\ell_2:P_2 + t\\mathbf{d}_2$ is $d = (P_2 - P_1) \\cdot \\hat{\\mathbf{n}}$ where $\\mathbf{n} = \\mathbf{d}_1 \\cdot \\mathbf{d}_2$.\n",
    "\n",
    "- [**Shortest distance between a point and a plane**](ex4.7): The shortest distance between the point $Q$ and the plane which passes through the point $P$ and has norm vector $\\mathbf{n}$ is $d = (Q - P)\\cdot \\hat{\\mathbf{n}}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Vector spaces\n",
    "\n",
    "A [**vector space**](vector-space-definition) over a field $F$ is a non-empty set $V$ of which the following operations are defined:\n",
    "\n",
    "- **addition**: If $\\mathbf{u}, \\mathbf{v} \\in V$ then $\\mathbf{u} + \\mathbf{v} \\in V$.\n",
    "\n",
    "- **scalar multiplication**: If $\\mathbf{u} \\in V$ and $\\alpha \\in F$ then $\\alpha \\mathbf{u} \\in V$.\n",
    "\n",
    "### Axioms of vector spaces\n",
    "\n",
    "For $V$ to be a vector space over $F$ and $\\alpha, \\beta \\in F$ the following eight [**axioms**](vector-space-axioms-theorem) must hold\n",
    "\n",
    "- A1: Associativity of vector addition: $\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}$;\n",
    "- A2: Commutativity of vector addition: $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$;\n",
    "- A3: Identity element of vector addition: there exists an element $\\mathbf{0} \\in V$ such that $\\mathbf{u} + \\mathbf{0} = \\mathbf{u}$ for all $\\mathbf{v} \\in V$;\n",
    "- A4: Additive inverse: For every $\\mathbf{u} \\in V$ there exists an element $- \\mathbf{v} \\in V$ such that $\\mathbf{u} + (- \\mathbf{u}) = \\mathbf{0}$;\n",
    "- M1: Associativity of scalar multiplication: $\\alpha(\\beta \\mathbf{u}) = (\\alpha \\beta) \\mathbf{u}$;\n",
    "- M2: Identity element of scalar multiplication: there exists an element $1$ such that $1 \\mathbf{u} = \\mathbf{u}$;\n",
    "- M3: Distributivity of scalar multiplication with respect to vector addition: $\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha \\mathbf{u} + \\alpha \\mathbf{v}$;\n",
    "- M4: Distributivity of scalar multiplication with respect to addition: $(\\alpha + \\beta)\\mathbf{u} = \\alpha \\mathbf{u} + \\beta \\mathbf{u}$.\n",
    "\n",
    "### Subspaces\n",
    "\n",
    "If $V$ is a vector space and $W \\subset V$ then $W$ is a [**subspace**](subspace-definition) if $\\mathbf{u} + \\alpha \\mathbf{w} \\in W$ where $\\mathbf{u}, \\mathbf{w} \\in W$ and $\\alpha \\in F$. \n",
    "\n",
    "### Linear dependence\n",
    "\n",
    "For the following let $V$ be a vector space over $F$ and $W$ a subspace of $V$.\n",
    "\n",
    "- [**Linear combination**](linear-combination-definition): For $\\mathbf{u} \\in V$ there exists $\\mathbf{v}_i \\in W$ and $\\alpha_i \\in F$ such that $\\mathbf{u} = \\alpha_1 \\mathbf{v}_1 + \\alpha_2 \\mathbf{v}_2 + \\cdots + \\alpha_n \\mathbf{v}_n$.\n",
    "- [**Linear independence**](linear-dependence-definition): Let $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\in V$ then if $\\alpha_1 \\mathbf{v}_1 + \\alpha_2 \\mathbf{v}_2 + \\cdots + \\alpha_n \\mathbf{v}_n = \\mathbf{0}$ only has the solution $\\alpha_i = 0$ then $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\in V$ are linearly independent over $F$.\n",
    "- [**Linear dependence**](linear-dependence-definition): IF the linear combination of vectors above is satisfied for at least one non-zero $\\alpha_i$ then $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\in V$ are linearly dependent over $F$.\n",
    "\n",
    "### Basis\n",
    "\n",
    "- [**Spanning set**](spanning-set-definition): Let $V$ be a vector space over $F$ and $W$ is a subspace of $V$ such that $\\mathbf{u} \\in W$ are expressible as a linear combination of $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\in V$ then $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$ is a spanning set for $W$.\n",
    "\n",
    "- [**Basis**](basis-definition): A basis of a vector space $V$ over $F$ is a linearly independent spnning subset of $V$.\n",
    "\n",
    "- [**Orthogonal basis**](orthogonal-basis-definition): A basis where each of the vectors in the basis are orthogonal to each other.\n",
    "\n",
    "- [**Orthonormal basis**](orthonormal-basis-definition): An orthogonal basis where each of the basis vector is a unit vector.\n",
    "\n",
    "- [**Dimensional of a vector space**](vector-space-dimension-definition): The number of elements in the basis for the vector space.\n",
    "\n",
    "- [**Standard basis**](change-of-basis-section): The standard basis for $\\mathbb{R}^n$ is $E = \\{ \\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n \\}$ where $\\mathbf{e}_i$ is column $i$ of the identity matrix.\n",
    "\n",
    "- [**Change of basis**](change-of-basis-example): The vector $[\\mathbf{u}]_E$ represented with respect to another basis $W= \\{ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\}$ is $[\\mathbf{u}]_W = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_n)$ such that $\\alpha_1 \\mathbf{v}_1 + \\alpha_2 \\mathbf{v}_2 + \\cdots + \\alpha_n \\mathbf{v}_n = \\mathbf{u}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Linear transformations\n",
    "\n",
    "If $V$ and $W$ are two vectors spaces over $F$ then a [**linear transformation**](linear-transformation-definition) is a mapping $T: V \\to W$ such that for $x,y \\in V$ and $\\alpha \\in F$ the following hold\n",
    "\n",
    "- **Addition**: $T(x + y) = T(x) + T(y)$.\n",
    "- **Scalar multiplication**: $T(\\alpha x) = \\alpha T(x)$.\n",
    "\n",
    "We can combine these two conditions to give $T(x + \\alpha y) = T(x) + \\alpha T(y)$.\n",
    "\n",
    "- [**Transformation matrix**](transformation-matrix-definition): Let $T:V \\to W$ be a linear transformation and $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}$ is a basis of $V$ then the transformation matrix is the matrix $A$ such that $T(\\mathbf{u}) = A \\mathbf{u}$ where $A = (T(\\mathbf{v}_1), T(\\mathbf{v}_2), \\ldots, T(\\mathbf{v}_n))$.\n",
    "\n",
    "- [**Determining the transformation matrix given the image**](finding-transformation-matrix-theorem): Given a linear transformation $\\mathbb{R}^n \\to \\mathbb{R}^n$ applied to a set of vectors $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n\\}$ then $A = (T(\\mathbf{u}_1), T(\\mathbf{u}_2), \\ldots, T(\\mathbf{u}_n)) \\cdot (\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n)^{-1}$.\n",
    "\n",
    "- [**Inverse transformation**](inverse-transformation-definition): Let $T:V \\to W$ be a linear transformation then its inverse is $T^{-1}: W \\to V$.\n",
    "\n",
    "- [**Inverse transformation matrix**](inverse-transformation-definition): If $A$ is a transformation matrix for $T$ then $A^{-1}$ is the transformation matrix for $T^{-1}$.\n",
    "\n",
    "- [**Composite transformation**](composite-transformation-definition): Let $S:V \\to W$ and $T:W \\to X$ be two linear transformations then the composition of $S$ and $T$ is $S\\circ T: V \\to X$ defined by $S(T(\\mathbf{u}))$ for $\\mathbf{u} \\in V$.\n",
    "\n",
    "- [**Composite transformation matrices**](composite-transformation-matrices-theorem): If $A$ and $B$ are the transformation matrices for $T$ and $S$ respectively then $S(T(\\mathbf{u})) = BA \\mathbf{u}$.\n",
    "\n",
    "- [**Homogeneous co-ordinates**](homogeneous-coordinates-definition): The homogeneous co-ordinates of a point $\\mathbf{u}$ in $\\mathbf{R}^n$ expressed using homogeneous co-ordinates is $\\mathbf{u} = (\\lambda u_1, \\lambda u_2, \\ldots, \\lambda u_n, \\lambda)$. Note that $(x, y, z)$ in Cartesian co-ordinates is the same as $(x, y, z, 1)$ in homogeneous co-ordinates.\n",
    "\n",
    "- [**Translation**](translation-definition): The transformation matrix to translate a vector in $\\mathbb{R}^n$ expressed in homogeneous co-ordinates by a translation vector $\\mathbf{t}$ is \n",
    "\n",
    "$${T}(\\mathbf{t}) = \\begin{pmatrix} & & & t_1 \\\\  & I & & \\vdots \\\\ & & & t_n \\\\ 0 & \\cdots & 0 & 1 \\end{pmatrix}.$$\n",
    "\n",
    "- [**Rotation**](rotation-in-R2-theorem): The transformation matrix to rotate  a vector in $\\mathbb{R}^2$ expressed in homogeneous co-ordinates by $\\theta$ anti-clockwise about the origin is \n",
    "\n",
    "$${Rot}(\\theta) = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\ \\sin(\\theta) & \\cos(\\theta) & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.$$\n",
    "\n",
    "- [**Reflection**](reflection-theorem):  The transformation matrix to reflect  a vector  in $\\mathbb{R}^2$ expressed in homogeneous co-ordinates about a line that passes through the origin and makes an angle $\\theta$ with the $x$-axis is \n",
    "\n",
    "$${Ref}(\\theta) = \\begin{pmatrix} \\cos(2\\theta) & \\sin(2\\theta) & 0 \\\\ \\sin(2\\theta) & -\\cos(2\\theta) & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.$$\n",
    "\n",
    "- [**Scaling**](scaling-definition): The transformation matrix to scale  a vector  in $\\mathbb{R}^n$ expressed in homogeneous co-ordinates by the scaling vector \n",
    "\n",
    "$$\\mathbf{s}$ is ${S}(\\mathbf{s}) = \\begin{pmatrix} s_1 & 0 & \\cdots & 0 \\\\ 0 & s_2 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & 0 & 1 \\end{pmatrix}.$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
